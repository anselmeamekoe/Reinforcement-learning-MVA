import sys
sys.path.insert(0, './utils')
import numpy as np
import matplotlib.pyplot as plt
import math
from cliffwalk import CliffWalk
import time
#import sys


def policy_evaluation(P, R, policy, gamma=0.9, tol=1e-2):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        policy: np.array
            matrix mapping states to action (Ns)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        value_function: np.array
            The value function of the given policy
    """
    Ns, Na = R.shape
    # ====================================================
	# YOUR IMPLEMENTATION HERE 
    # 
    value_function = np.zeros(Ns)
    r_pi = np.zeros(Ns)
    p_pi = np.zeros((Ns,Ns))
    for s in range(Ns):
        a = policy[s]
        r_pi[s] = R[s,a]
        for s_next in range(Ns):
            p_pi[s,s_next] = P[s,a,s_next]
    value_function = np.linalg.solve(np.eye(Ns)- gamma*p_pi, r_pi)
    # ====================================================
    return value_function

def policy_iteration(P, R, gamma=0.9, tol=1e-3):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        policy: np.array
            the final policy
        V: np.array
            the value function associated to the final policy
    """
    Ns, Na = R.shape
    V = np.zeros(Ns)
    policy = np.zeros(Ns, dtype=np.int)
    # ====================================================
	# YOUR IMPLEMENTATION HERE 
    #
    policy_old = policy.copy()
    V_old = policy_evaluation(P, R, policy_old)
    V = policy_evaluation(P, R, policy)
    it = 0
    tp = time.time()
    while (np.linalg.norm(V - V_old, np.inf)>tol or it == 0 ):
        policy_old = policy.copy()
        V_old= V.copy()
        it += 1 
        for s in range(Ns):
            policy[s] = np.argmax(R[s,:] + gamma*P[s,:,:].dot(V))
        V = policy_evaluation(P, R, policy,gamma= gamma)   
    tp = time.time() - tp 
    # we return also the nuumber of iteration  and the time 
    # and the time of execution tp 
    # ====================================================
    return policy, V, it , tp 



def value_iteration(P, R, gamma=0.9, tol=1e-3):
    """
    Args:
        P: np.array
            transition matrix (NsxNaxNs)
        R: np.array
            reward matrix (NsxNa)
        gamma: float
            discount factor
        tol: float
            precision of the solution
    Return:
        Q: final Q-function (at iteration n)
        greedy_policy: greedy policy wrt Qn
        Qfs: all Q-functions generated by the algorithm (for visualization)
    """
    Ns, Na = R.shape
    Q = np.zeros((Ns, Na))
    Qfs = [Q]
    # ====================================================
	# YOUR IMPLEMENTATION HERE 
    # 
    greedy_policy = np.zeros(Ns)
    V_old = np.zeros(Ns)
    V = np.zeros(Ns)
    it = 0
    tp = time.time()
    # np.abs(V-V_old).max()>tol
    while ( np.linalg.norm(V-V_old, np.inf)>tol or it ==0):
        V_old = V.copy()
        it +=1
        for s in range(Ns):
            for a in range(Na):
                Q[s,a] = R[s,a]+ gamma*(P[s,a,:].dot(V))
            V[s] = np.max(Q[s,:])
        Qfs.append(Q.copy())
    tp = time.time() - tp 
    greedy_policy = np.argmax(Q,axis = 1)
    Qfs[0] = np.zeros((Ns, Na))
    
    #  we return  in addition  the number of iteration it   and the time tp 
    # ====================================================
    return Q, greedy_policy, Qfs, it, tp 


# Edit below to run policy and value iteration on different environments and
# visualize the resulting policies in action!
# You may change the parameters in the functions below
if __name__ == "__main__":
    tol =1e-5
    #env = CliffWalk(proba_succ=1)
    env = CliffWalk(proba_succ= 0.9)
    print(env.R.shape)
    print(env.P.shape)
    env.render()

    # run value iteration to obtain Q-values
    VI_Q, VI_greedypol, all_qfunctions, VI_it, VI_tp = value_iteration(env.P, env.R, gamma=env.gamma, tol=tol)

    # render the policy
    print("[VI]Greedy policy: ")
    env.render_policy(VI_greedypol)

    # compute the value function of the greedy policy using matrix inversion
    #greedy_V = np.zeros((env.Ns, env.Na))
    greedy_V = np.zeros(env.Ns)
    # ====================================================
	# YOUR IMPLEMENTATION HERE 
    # compute value function of the greedy policy
    
    # we use matrix inversion in policy_evaluation.
    greedy_V = policy_evaluation(env.P,env.R, VI_greedypol, gamma= env.gamma, tol= tol )

    # ====================================================

    # show the error between the computed V-functions and the final V-function
    # (that should be the optimal one, if correctly implemented)
    # as a function of time
    norms = [ np.linalg.norm(q.max(axis=1) - greedy_V) for q in all_qfunctions]
    
    plt.plot(norms)
    plt.xlabel('Iteration')
    plt.ylabel('Error')
    plt.title("Value iteration: convergence")

    #### POLICY ITERATION ####
    PI_policy, PI_V, PI_it, PI_tp = policy_iteration(env.P, env.R, gamma=env.gamma, tol=tol)
    print("\n[PI]final policy: ")
    env.render_policy(PI_policy)

    # control that everything is correct
    assert np.allclose(PI_policy, VI_greedypol),\
        "You should check the code, the greedy policy computed by VI is not equal to the solution of PI"
    assert np.allclose(PI_V, greedy_V),\
        "Since the policies are equal, even the value function should be"

    
    # for visualizing the execution of a policy, you can use the following code
    state = env.reset()
    env.render()
    for i in range(15):
        action = VI_greedypol[state]
        state, reward, done, _ = env.step(action)
        env.render()


    plt.show()